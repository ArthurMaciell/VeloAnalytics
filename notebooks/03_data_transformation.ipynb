{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f102a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c65814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Orçamento\\\\Desktop\\\\Bike Sales\\\\VeloAnalytics\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1b59c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efd560d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Orçamento\\\\Desktop\\\\Bike Sales\\\\VeloAnalytics'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49fb2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from src.logger_config import logger\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89959f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Transformation Configuration Entity ---\n",
    "# This defines the structure for the data transformation configuration.\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    output_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f873cfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f0307",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = Path(\"config.yaml\")):\n",
    "        \"\"\"\n",
    "        Initializes the ConfigurationManager by reading the main config file.\n",
    "        It also creates the main artifacts directory.\n",
    "        \"\"\"\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        create_directories([Path(self.config.artifacts_root)])\n",
    "        \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        \"\"\"\n",
    "        Extracts the data transformation configuration from the main config file.\n",
    "        \"\"\"\n",
    "        config = self.config.data_transformation\n",
    "        create_directories([Path(config.root_dir), Path(config.output_path)])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            data_path=Path(config.data_path),\n",
    "            output_path=Path(config.output_path)\n",
    "        )\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8788d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from src.logger_config import logger\n",
    "from src.utils import read_yaml\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        \"\"\"\n",
    "        Initializes the DataTransformation component with its configuration\n",
    "        and loads the data schema.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.schema = read_yaml(Path(\"schema.yaml\"))\n",
    "\n",
    "    def _clean_and_transform(self, df: pd.DataFrame, file_schema: dict) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Private helper method to apply cleaning and transformations to a dataframe.\n",
    "        \"\"\"\n",
    "        # --- 1. Enforce Data Types based on schema.yaml ---\n",
    "        for col, dtype in file_schema.items():\n",
    "            if col in df.columns:\n",
    "                if 'date' in col.lower() or 'at' in col.lower():\n",
    "                    df[col] = pd.to_datetime(df[col], format='%Y%m%d', errors='coerce')\n",
    "                else:\n",
    "                    # Use astype for other types, ignoring errors for robustness\n",
    "                    df[col] = df[col].astype(dtype, errors='ignore')\n",
    "        \n",
    "        # --- 2. Handle Missing Values ---\n",
    "        # Fill numeric columns with 0 and object (text) columns with 'N/A'\n",
    "        for col in df.select_dtypes(include=['number']).columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "        for col in df.select_dtypes(include=['object', 'string']).columns:\n",
    "            df[col] = df[col].fillna('N/A')\n",
    "            \n",
    "        return df\n",
    "\n",
    "    def validate_and_transform_data(self):\n",
    "        \"\"\"\n",
    "        Reads all raw CSV files, validates them against the defined schema,\n",
    "        applies transformations, and saves them as processed Parquet files.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            raw_data_path = self.config.data_path\n",
    "            processed_data_path = self.config.output_path\n",
    "            all_schemas = self.schema.COLUMNS\n",
    "\n",
    "            all_csv_files = [f for f in os.listdir(raw_data_path) if f.endswith('.csv')]\n",
    "            logger.info(f\"Found {len(all_csv_files)} CSV files to transform.\")\n",
    "\n",
    "            for csv_file in all_csv_files:\n",
    "                file_name = Path(csv_file).stem\n",
    "                \n",
    "                if file_name not in all_schemas:\n",
    "                    logger.warning(f\"Schema not defined for {csv_file}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                logger.info(f\"Processing and validating file: {csv_file}\")\n",
    "                \n",
    "                file_schema = all_schemas[file_name]\n",
    "                # --- FIX: Added encoding='latin1' to handle special characters ---\n",
    "                df = pd.read_csv(os.path.join(raw_data_path, csv_file), encoding='latin1')\n",
    "\n",
    "                # --- Schema Column Validation ---\n",
    "                schema_cols = set(file_schema.keys())\n",
    "                df_cols = set(df.columns)\n",
    "                \n",
    "                if not schema_cols.issubset(df_cols):\n",
    "                    missing_cols = schema_cols - df_cols\n",
    "                    logger.error(f\"Schema validation failed for {csv_file}. Missing columns: {missing_cols}\")\n",
    "                    continue\n",
    "                \n",
    "                # --- Apply Cleaning and Transformations ---\n",
    "                df_transformed = self._clean_and_transform(df, file_schema)\n",
    "\n",
    "                # --- Save the processed dataframe as a parquet file ---\n",
    "                output_file_path = os.path.join(processed_data_path, f\"{file_name}.parquet\")\n",
    "                df_transformed.to_parquet(output_file_path, index=False)\n",
    "                logger.info(f\"Successfully transformed and saved {csv_file} to {output_file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"An error occurred during data transformation: {e}\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "851bfa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-28 10:47:14,865: INFO: 159385861: >>>>>> Stage 'Data Transformation stage' started <<<<<<]\n",
      "[2025-08-28 10:47:14,870: INFO: utils: YAML file loaded successfully: config.yaml]\n",
      "[2025-08-28 10:47:14,872: INFO: utils: Directory created or already exists: artifacts]\n",
      "[2025-08-28 10:47:14,874: INFO: utils: Directory created or already exists: artifacts\\data_transformation]\n",
      "[2025-08-28 10:47:14,875: INFO: utils: Directory created or already exists: data\\02_processed]\n",
      "[2025-08-28 10:47:14,885: INFO: utils: YAML file loaded successfully: schema.yaml]\n",
      "[2025-08-28 10:47:14,886: INFO: 1008841359: Found 9 CSV files to transform.]\n",
      "[2025-08-28 10:47:14,886: INFO: 1008841359: Processing and validating file: Addresses.csv]\n",
      "[2025-08-28 10:47:14,890: ERROR: 1008841359: Schema validation failed for Addresses.csv. Missing columns: {'ADDRESSID'}]\n",
      "[2025-08-28 10:47:14,892: INFO: 1008841359: Processing and validating file: BusinessPartners.csv]\n",
      "[2025-08-28 10:47:14,913: INFO: 1008841359: Successfully transformed and saved BusinessPartners.csv to data\\02_processed\\BusinessPartners.parquet]\n",
      "[2025-08-28 10:47:14,913: INFO: 1008841359: Processing and validating file: Employees.csv]\n",
      "[2025-08-28 10:47:14,924: INFO: 1008841359: Successfully transformed and saved Employees.csv to data\\02_processed\\Employees.parquet]\n",
      "[2025-08-28 10:47:14,935: INFO: 1008841359: Processing and validating file: ProductCategories.csv]\n",
      "[2025-08-28 10:47:14,947: INFO: 1008841359: Successfully transformed and saved ProductCategories.csv to data\\02_processed\\ProductCategories.parquet]\n",
      "[2025-08-28 10:47:14,948: INFO: 1008841359: Processing and validating file: ProductCategoryText.csv]\n",
      "[2025-08-28 10:47:14,961: INFO: 1008841359: Successfully transformed and saved ProductCategoryText.csv to data\\02_processed\\ProductCategoryText.parquet]\n",
      "[2025-08-28 10:47:14,961: INFO: 1008841359: Processing and validating file: Products.csv]\n",
      "[2025-08-28 10:47:14,976: INFO: 1008841359: Successfully transformed and saved Products.csv to data\\02_processed\\Products.parquet]\n",
      "[2025-08-28 10:47:14,976: INFO: 1008841359: Processing and validating file: ProductTexts.csv]\n",
      "[2025-08-28 10:47:14,994: INFO: 1008841359: Successfully transformed and saved ProductTexts.csv to data\\02_processed\\ProductTexts.parquet]\n",
      "[2025-08-28 10:47:14,997: INFO: 1008841359: Processing and validating file: SalesOrderItems.csv]\n",
      "[2025-08-28 10:47:15,035: INFO: 1008841359: Successfully transformed and saved SalesOrderItems.csv to data\\02_processed\\SalesOrderItems.parquet]\n",
      "[2025-08-28 10:47:15,037: INFO: 1008841359: Processing and validating file: SalesOrders.csv]\n",
      "[2025-08-28 10:47:15,059: INFO: 1008841359: Successfully transformed and saved SalesOrders.csv to data\\02_processed\\SalesOrders.parquet]\n",
      "[2025-08-28 10:47:15,059: INFO: 159385861: >>>>>> Stage 'Data Transformation stage' completed successfully <<<<<<\n",
      "\n",
      "x==========x]\n"
     ]
    }
   ],
   "source": [
    "# --- STAGE 3: DATA TRANSFORMATION ---\n",
    "STAGE_NAME = \"Data Transformation stage\"\n",
    "try:\n",
    "    logger.info(f\">>>>>> Stage '{STAGE_NAME}' started <<<<<<\")\n",
    "            \n",
    "    # Initialize the configuration manager\n",
    "    config = ConfigurationManager()\n",
    "            \n",
    "    # Get the specific configuration for data transformation\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "            \n",
    "    # Initialize the data transformation component\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "            \n",
    "    # Run the transformation process\n",
    "    data_transformation.validate_and_transform_data()\n",
    "            \n",
    "    logger.info(f\">>>>>> Stage '{STAGE_NAME}' completed successfully <<<<<<\\n\\nx==========x\")\n",
    "except Exception as e:\n",
    "    logger.exception(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8f9be9",
   "metadata": {},
   "source": [
    "                # (Optional) Here you would add more transformation logic:\n",
    "                # - Enforce data types from schema\n",
    "                # - Handle missing values\n",
    "                # - Create new features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
