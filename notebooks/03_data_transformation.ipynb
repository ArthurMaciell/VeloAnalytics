{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f102a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51c65814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Orçamento\\\\Desktop\\\\Bike Sales\\\\VeloAnalytics\\\\notebooks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b59c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efd560d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Orçamento\\\\Desktop\\\\Bike Sales\\\\VeloAnalytics'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b49fb2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from src.logging import logger\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89959f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Transformation Configuration Entity ---\n",
    "# This defines the structure for the data transformation configuration.\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    output_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f873cfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a7f0307",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = Path(\"config.yaml\")):\n",
    "        \"\"\"\n",
    "        Initializes the ConfigurationManager by reading the main config file.\n",
    "        It also creates the main artifacts directory.\n",
    "        \"\"\"\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        create_directories([Path(self.config.artifacts_root)])\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        \"\"\"\n",
    "        Extracts the data transformation configuration from the main config file.\n",
    "        \"\"\"\n",
    "        config = self.config.data_transformation\n",
    "        create_directories([Path(config.root_dir), Path(config.output_path)])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            data_path=Path(config.data_path),\n",
    "            output_path=Path(config.output_path)\n",
    "        )\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa8788d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from src.logging import logger\n",
    "from src.utils import read_yaml\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        \"\"\"\n",
    "        Initializes the DataTransformation component with its configuration.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.schema = read_yaml(Path(\"schema.yaml\"))\n",
    "\n",
    "    def _clean_and_transform(self, df: pd.DataFrame, schema: dict) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Private helper method to apply cleaning and transformations to a dataframe.\n",
    "        \"\"\"\n",
    "        # --- 1. Drop Corrupted \"Unnamed\" Columns ---\n",
    "        unnamed_cols = [col for col in df.columns if 'unnamed' in col.lower()]\n",
    "        if unnamed_cols:\n",
    "            df = df.drop(columns=unnamed_cols)\n",
    "            logger.info(f\"Dropped unnamed columns: {unnamed_cols}\")\n",
    "\n",
    "        # --- 2. Enforce Data Types ---\n",
    "        for col, dtype in schema.items():\n",
    "            if col in df.columns:\n",
    "                if 'date' in col.lower() or 'at' in col.lower():\n",
    "                    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                else:\n",
    "                    df[col] = df[col].astype(dtype, errors='ignore')\n",
    "        \n",
    "        # --- 3. Handle Missing Values (Updated to avoid 'inplace=True') ---\n",
    "        for col in df.select_dtypes(include=['number']).columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "        for col in df.select_dtypes(include=['object']).columns:\n",
    "            df[col] = df[col].fillna('N/A')\n",
    "            \n",
    "        return df\n",
    "\n",
    "    def validate_and_transform_data(self):\n",
    "        \"\"\"\n",
    "        Reads all raw CSV files, validates them against the defined schema,\n",
    "        applies transformations, and saves them as processed Parquet files.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data_files = os.listdir(self.config.data_path)\n",
    "            csv_files = [f for f in data_files if f.endswith('.csv')]\n",
    "            logger.info(f\"Found {len(csv_files)} CSV files to transform.\")\n",
    "\n",
    "            for csv_file in csv_files:\n",
    "                file_name = Path(csv_file).stem\n",
    "                \n",
    "                if file_name not in self.schema.COLUMNS:\n",
    "                    logger.warning(f\"Schema not defined for {csv_file}. Skipping this file.\")\n",
    "                    continue\n",
    "\n",
    "                logger.info(f\"Processing and validating file: {csv_file}\")\n",
    "                \n",
    "                file_schema = self.schema.COLUMNS[file_name]\n",
    "                df = pd.read_csv(os.path.join(self.config.data_path, csv_file), encoding='latin1')\n",
    "\n",
    "                # --- ROBUST FIX: Clean column names to remove BOM and other issues ---\n",
    "                # The BOM character 'ï»¿' is sometimes read as part of the first column name.\n",
    "                # This explicitly removes it.\n",
    "                df.columns = df.columns.str.replace('ï»¿', '', regex=False).str.strip()\n",
    "\n",
    "                # Validate columns exist\n",
    "                validation_errors = [col for col in file_schema.keys() if col not in df.columns]\n",
    "                if validation_errors:\n",
    "                    logger.error(f\"Schema validation failed for {csv_file}. Missing columns: {validation_errors}\")\n",
    "                    continue\n",
    "                \n",
    "                # Apply cleaning and transformations\n",
    "                df_transformed = self._clean_and_transform(df, file_schema)\n",
    "\n",
    "                # Save the processed dataframe as a parquet file\n",
    "                output_file_path = os.path.join(self.config.output_path, f\"{file_name}.parquet\")\n",
    "                df_transformed.to_parquet(output_file_path, index=False)\n",
    "                logger.info(f\"Successfully transformed and saved {csv_file} to {output_file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"An error occurred during data transformation: {e}\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851bfa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-27 17:23:44,860: INFO: 4195453137: >>>>>> Stage 'Data Validation stage' started <<<<<<]\n",
      "[2025-08-27 17:23:44,863: INFO: utils: YAML file loaded successfully: config.yaml]\n",
      "[2025-08-27 17:23:44,865: INFO: utils: Directory created or already exists: artifacts]\n",
      "[2025-08-27 17:23:44,867: INFO: utils: Directory created or already exists: artifacts\\data_transformation]\n",
      "[2025-08-27 17:23:44,869: INFO: utils: Directory created or already exists: data\\02_processed]\n",
      "[2025-08-27 17:23:44,878: INFO: utils: YAML file loaded successfully: schema.yaml]\n",
      "[2025-08-27 17:23:44,880: INFO: 2255547456: Found 9 CSV files to transform.]\n",
      "[2025-08-27 17:23:44,881: INFO: 2255547456: Processing and validating file: Addresses.csv]\n",
      "[2025-08-27 17:23:44,904: INFO: 2255547456: Successfully transformed and saved Addresses.csv to data\\02_processed\\Addresses.parquet]\n",
      "[2025-08-27 17:23:44,905: INFO: 2255547456: Processing and validating file: BusinessPartners.csv]\n",
      "[2025-08-27 17:23:44,921: INFO: 2255547456: Successfully transformed and saved BusinessPartners.csv to data\\02_processed\\BusinessPartners.parquet]\n",
      "[2025-08-27 17:23:44,922: INFO: 2255547456: Processing and validating file: Employees.csv]\n",
      "[2025-08-27 17:23:44,929: INFO: 2255547456: Dropped unnamed columns: ['Unnamed: 13', 'Unnamed: 14', 'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18']]\n",
      "[2025-08-27 17:23:44,943: INFO: 2255547456: Successfully transformed and saved Employees.csv to data\\02_processed\\Employees.parquet]\n",
      "[2025-08-27 17:23:44,945: INFO: 2255547456: Processing and validating file: ProductCategories.csv]\n",
      "[2025-08-27 17:23:44,957: INFO: 2255547456: Successfully transformed and saved ProductCategories.csv to data\\02_processed\\ProductCategories.parquet]\n",
      "[2025-08-27 17:23:44,957: INFO: 2255547456: Processing and validating file: ProductCategoryText.csv]\n",
      "[2025-08-27 17:23:44,970: INFO: 2255547456: Successfully transformed and saved ProductCategoryText.csv to data\\02_processed\\ProductCategoryText.parquet]\n",
      "[2025-08-27 17:23:44,971: INFO: 2255547456: Processing and validating file: Products.csv]\n",
      "[2025-08-27 17:23:44,996: INFO: 2255547456: Successfully transformed and saved Products.csv to data\\02_processed\\Products.parquet]\n",
      "[2025-08-27 17:23:44,997: INFO: 2255547456: Processing and validating file: ProductTexts.csv]\n",
      "[2025-08-27 17:23:45,011: INFO: 2255547456: Successfully transformed and saved ProductTexts.csv to data\\02_processed\\ProductTexts.parquet]\n",
      "[2025-08-27 17:23:45,012: INFO: 2255547456: Processing and validating file: SalesOrderItems.csv]\n",
      "[2025-08-27 17:23:45,042: INFO: 2255547456: Successfully transformed and saved SalesOrderItems.csv to data\\02_processed\\SalesOrderItems.parquet]\n",
      "[2025-08-27 17:23:45,043: INFO: 2255547456: Processing and validating file: SalesOrders.csv]\n",
      "[2025-08-27 17:23:45,066: INFO: 2255547456: Successfully transformed and saved SalesOrders.csv to data\\02_processed\\SalesOrders.parquet]\n",
      "[2025-08-27 17:23:45,068: INFO: 4195453137: >>>>>> Stage 'Data Validation stage' completed successfully <<<<<<\n",
      "\n",
      "x==========x]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Orçamento\\AppData\\Local\\Temp\\ipykernel_22216\\2255547456.py:29: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Orçamento\\AppData\\Local\\Temp\\ipykernel_22216\\2255547456.py:29: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Orçamento\\AppData\\Local\\Temp\\ipykernel_22216\\2255547456.py:29: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Orçamento\\AppData\\Local\\Temp\\ipykernel_22216\\2255547456.py:29: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Orçamento\\AppData\\Local\\Temp\\ipykernel_22216\\2255547456.py:29: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Orçamento\\AppData\\Local\\Temp\\ipykernel_22216\\2255547456.py:29: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# --- STAGE 3: DATA TRANSFORMATION ---\n",
    "STAGE_NAME = \"Data Transformation stage\"\n",
    "try:\n",
    "    logger.info(f\">>>>>> Stage '{STAGE_NAME}' started <<<<<<\")\n",
    "            \n",
    "    # Initialize the configuration manager\n",
    "    config = ConfigurationManager()\n",
    "            \n",
    "    # Get the specific configuration for data transformation\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "            \n",
    "    # Initialize the data transformation component\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "            \n",
    "    # Run the transformation process\n",
    "    data_transformation.validate_and_transform_data()\n",
    "            \n",
    "    logger.info(f\">>>>>> Stage '{STAGE_NAME}' completed successfully <<<<<<\\n\\nx==========x\")\n",
    "except Exception as e:\n",
    "    logger.exception(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8f9be9",
   "metadata": {},
   "source": [
    "                # (Optional) Here you would add more transformation logic:\n",
    "                # - Enforce data types from schema\n",
    "                # - Handle missing values\n",
    "                # - Create new features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
