{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba96d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5972013a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Orçamento\\\\Desktop\\\\Bike Sales\\\\VeloAnalytics\\\\notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b833a9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30151822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Orçamento\\\\Desktop\\\\Bike Sales\\\\VeloAnalytics'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a471af65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from src.logging import logger\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98661e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Modelling Configuration Entity ---\n",
    "# This defines the structure for the data modelling configuration.\n",
    "@dataclass(frozen=True)\n",
    "class DataModellingConfig:\n",
    "    root_dir: Path\n",
    "    processed_data_path: Path\n",
    "    presentation_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1012f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = Path(\"config.yaml\")):\n",
    "        \"\"\"\n",
    "        Initializes the ConfigurationManager by reading the main config file.\n",
    "        It also creates the main artifacts directory.\n",
    "        \"\"\"\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        create_directories([Path(self.config.artifacts_root)])\n",
    "        \n",
    "    def get_data_modelling_config(self) -> DataModellingConfig:\n",
    "        \"\"\"\n",
    "        Extracts the data modelling configuration from the main config file.\n",
    "        \"\"\"\n",
    "        config = self.config.data_modelling\n",
    "        create_directories([Path(config.root_dir), Path(config.presentation_path)])\n",
    "\n",
    "        data_modelling_config = DataModellingConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            processed_data_path=Path(config.processed_data_path),\n",
    "            presentation_path=Path(config.presentation_path)\n",
    "        )\n",
    "        return data_modelling_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a87b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModelling:\n",
    "    def __init__(self, config: DataModellingConfig):\n",
    "        \"\"\"\n",
    "        Initializes the DataModelling component with its configuration.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "    def _load_processed_data(self) -> dict:\n",
    "        \"\"\"\n",
    "        Loads all processed Parquet files into a dictionary of pandas DataFrames.\n",
    "        \"\"\"\n",
    "        dataframes = {}\n",
    "        path = self.config.processed_data_path\n",
    "        for file_name in os.listdir(path):\n",
    "            if file_name.endswith('.parquet'):\n",
    "                table_name = Path(file_name).stem\n",
    "                dataframes[table_name] = pd.read_parquet(os.path.join(path, file_name))\n",
    "        logger.info(f\"Loaded {len(dataframes)} processed tables.\")\n",
    "        return dataframes\n",
    "\n",
    "    def build_star_schema(self):\n",
    "        \"\"\"\n",
    "        Builds the fact and dimension tables for the star schema.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting the data modelling process to build the star schema.\")\n",
    "            df_dict = self._load_processed_data()\n",
    "\n",
    "            # --- 1. Build dim_customer ---\n",
    "            df_partners = df_dict['BusinessPartners']\n",
    "            df_addresses = df_dict['Addresses']\n",
    "            dim_customer = pd.merge(df_partners, df_addresses, on='ADDRESSID', how='left')\n",
    "            \n",
    "            # --- 2. Build dim_product (Now with ProductTexts) ---\n",
    "            df_products = df_dict['Products']\n",
    "            df_prod_cat_text = df_dict['ProductCategoryText']\n",
    "            df_prod_text = df_dict['ProductTexts']\n",
    "            \n",
    "            # Filter for English descriptions for consistency\n",
    "            df_prod_cat_text = df_prod_cat_text[df_prod_cat_text['LANGUAGE'] == 'EN']\n",
    "            df_prod_text = df_prod_text[df_prod_text['LANGUAGE'] == 'EN']\n",
    "            \n",
    "            # First join: Products with Category Text\n",
    "            dim_product_intermediate = pd.merge(df_products, df_prod_cat_text, on='PRODCATEGORYID', how='left')\n",
    "            # Second join: Result with Product Text\n",
    "            dim_product = pd.merge(dim_product_intermediate, df_prod_text, on='PRODUCTID', how='left')\n",
    "\n",
    "\n",
    "            # --- 3. Build dim_date ---\n",
    "            df_sales_orders = df_dict['SalesOrders']\n",
    "            df_sales_orders['CREATEDAT'] = pd.to_datetime(df_sales_orders['CREATEDAT'])\n",
    "            min_date = df_sales_orders['CREATEDAT'].min()\n",
    "            max_date = df_sales_orders['CREATEDAT'].max()\n",
    "            \n",
    "            dim_date = pd.DataFrame({'Date': pd.date_range(min_date, max_date)})\n",
    "            dim_date['Year'] = dim_date['Date'].dt.year\n",
    "            dim_date['Month'] = dim_date['Date'].dt.month\n",
    "            dim_date['Day'] = dim_date['Date'].dt.day\n",
    "            dim_date['Quarter'] = dim_date['Date'].dt.quarter\n",
    "            dim_date['DayOfWeek'] = dim_date['Date'].dt.dayofweek # Monday=0, Sunday=6\n",
    "\n",
    "            # --- 4. Build fact_sales ---\n",
    "            df_sales_items = df_dict['SalesOrderItems']\n",
    "            # The fact table is based on the items, as it's the lowest grain\n",
    "            fact_sales = pd.merge(df_sales_items, df_sales_orders[['SALESORDERID', 'PARTNERID', 'CREATEDAT']], on='SALESORDERID', how='left')\n",
    "            fact_sales.rename(columns={'CREATEDAT': 'OrderDate'}, inplace=True)\n",
    "            \n",
    "            # --- 5. Save Presentation Tables ---\n",
    "            presentation_path = self.config.presentation_path\n",
    "            dim_customer.to_parquet(os.path.join(presentation_path, \"dim_customer.parquet\"), index=False)\n",
    "            dim_product.to_parquet(os.path.join(presentation_path, \"dim_product.parquet\"), index=False)\n",
    "            dim_date.to_parquet(os.path.join(presentation_path, \"dim_date.parquet\"), index=False)\n",
    "            fact_sales.to_parquet(os.path.join(presentation_path, \"fact_sales.parquet\"), index=False)\n",
    "            \n",
    "            logger.info(f\"Successfully built and saved star schema tables to '{presentation_path}'\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during data modelling: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6f0da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STAGE 4: DATA MODELLING ---\n",
    "STAGE_NAME = \"Data Modelling stage\"\n",
    "try:\n",
    "    logger.info(f\">>>>>> Stage '{STAGE_NAME}' started <<<<<<\")\n",
    "            \n",
    "    # Initialize the configuration manager\n",
    "    config = ConfigurationManager()\n",
    "            \n",
    "    # Get the specific configuration for data modelling\n",
    "    data_modelling_config = config.get_data_modelling_config()\n",
    "            \n",
    "    # Initialize the data modelling component with the configuration\n",
    "    data_modelling = DataModelling(config=data_modelling_config)\n",
    "            \n",
    "    # Run the star schema building process\n",
    "    data_modelling.build_star_schema()\n",
    "            \n",
    "    logger.info(f\">>>>>> Stage '{STAGE_NAME}' completed successfully <<<<<<\\n\\nx==========x\")\n",
    "except Exception as e:\n",
    "    logger.exception(e)\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
